This response provides the most minimal solution for each relevant question, along with the formulas used for numerical calculations, drawing solely upon the provided sources. The questions originate from the back exercises of Chapters 3, 4, 5, 6, and 7.

---

### Chapter 3: Processes (3.1, 3.2, 3.3)

| Question | Minimal Solution & Formula/Principle                                                                                                                                                                                                                                                                                                                                                                                | Citation |
| :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------- |
| **3.1**  | **Output at LINE A is 5.** The `fork()` operation creates a child process that receives a copy of the parent's data space, including the global variable `value`. The child modifies its _copy_ (5 + 15 = 20), but the parent's original variable (5) remains unchanged, and the parent prints this original value.                                                                                                 |          |
| **3.2**  | **8 processes.**                                                                                                                                                                                                                                                                                                                                                                                                    |          |
|          | _Formula/Principle:_ When a process calls `fork()` $N$ times, assuming all calls succeed, the total number of processes created (including the initial parent) is $2^N$. Here, $N=3$ calls to `fork()`: $2^3 = 8$.                                                                                                                                                                                                  |          |
| **3.3**  | Three major complications are **data inconsistency** (due to concurrent access to shared data), the requirement for mechanisms for **process synchronization**, and the need for **deadlock handling**.                                                                                                                                                                                                             |          |
| **3.4**  | If the new context is **already loaded** into a register set, the context switch involves simply changing the hardware pointer/register that designates the current register set, making the switch **very fast**. If the new context is in memory, the kernel must first **save one of the old register sets** to memory and then **load the new context** from memory, which is a significantly slower operation. |          |
| **3.5**  | Only **Shared memory segments** are typically shared between the parent and child processes created by `fork()`. (The stack and heap generally receive copies, not shared memory).                                                                                                                                                                                                                                  |          |
| **3.8**  | **Short-term scheduling** selects processes ready to execute (CPU scheduling). **Medium-term scheduling** handles swapping processes into or out of memory (influencing the degree of multiprogramming). **Long-term scheduling** selects initial jobs/processes from storage to load into the ready queue/memory.                                                                                                  |          |
| **3.9**  | The kernel **saves the state** of the old process (including registers, program counter, CPU scheduling information, memory management information, and I/O status) into its Process Control Block (PCB), and then **reloads the saved state** from the PCB of the new process.                                                                                                                                     |          |
| **3.12** | **16 processes.**                                                                                                                                                                                                                                                                                                                                                                                                   |          |
|          | _Formula/Principle:_ If `fork()` is called $N$ times sequentially within a loop, the total number of processes created (including the initial parent) is $2^N$. Here, $N=4$ calls to `fork()`: $2^4 = 16$.                                                                                                                                                                                                          |          |
| **3.17** | **Line X Output:** The child prints the _modified values_ of the array. **Line Y Output:** The parent prints the _original values_ of the array. This is because the child process receives a copy of the parent’s address space when `fork()` executes, and modifications by the child (Line X) are local to the child.                                                                                            |          |

### Chapter 4: Threads (4.1, 4.2, 4.3, 4.4, 4.4.1)

|Question|Minimal Solution & Formula/Principle|Citation|
|:--|:--|:--|
|**4.6**|Two examples: 1. A problem that is **inherently sequential** and cannot be divided into separate, concurrent tasks. 2. Any solution running on a single-processor system where the overhead of **context switching** between threads outweighs any benefits.||
|**4.7**|A multithreaded solution provides better performance on a single-processor system when threads frequently perform **blocking system calls** or require **I/O waits**. When one thread blocks, the kernel scheduler can quickly switch to another ready kernel thread, maximizing CPU utilization.||
|**4.8**|**b. Heap memory** and **c. Global variables** are shared across threads in a multithreaded process.||
|**4.9**|**No.** User-level threads are managed by the thread library and the kernel is unaware of them. In systems using the many-to-one model, if one user thread makes a blocking system call, it blocks the entire process and all other user threads associated with it, regardless of the number of available processors.||
|**4.11**|**Yes.** Concurrency without parallelism occurs on **single-processor systems** (or systems with fewer processing cores than processes). Concurrency is achieved through the CPU scheduler rapidly switching between processes or threads (time sharing), interleaving their execution.||
|**4.12**|(a) **1.43**; (b) **1.82**.||
||_Formula/Principle:_ **Amdahl’s Law** is used: $Speedup \le \frac{1}{S + \frac{(1-S)}{N}}$, where $S$ is the serial component (40% or 0.40) and $N$ is the number of processing cores.||
||(a) N=2: $Speedup \le \frac{1}{0.40 + \frac{0.60}{2}} \approx \textbf{1.43}$||
||(b) N=4: $Speedup \le \frac{1}{0.40 + \frac{0.60}{4}} \approx \textbf{1.82}$||
|**4.13**|**Multithreaded statistical program (Ex 4.21):** **Task parallelism** (different functions performed concurrently, e.g., calculating average, max, min). **Sudoku validator (Project 1):** **Task parallelism** (separate threads check rows, columns, and 3x3 subgrids). **Multithreaded sorting program (Project 2):** **Data parallelism** (the sorting threads apply the same sorting function to different subsets of data).||
|**4.14**|The application should use **separate threads** for **computationally intensive** portions and **I/O intensive** portions. This allows the operating system to exploit parallelism on multicore systems for computation while overlapping the computation with the time spent waiting for I/O completion.||
|**4.15**|(a) **4 unique processes.** (b) **5 unique threads.** (P0 forks P1. P1 forks P2 and creates a second thread. P0 forks P3. Total unique processes: P0, P1, P2, P3. Total unique threads: T0, T1a, T1b, T2, T3).||

### Chapter 5: Process Synchronization (5.1, 5.2, 5.3, 5.4, 5.7)

|Question|Minimal Solution & Formula/Principle|Citation|
|:--|:--|:--|
|**5.3**|**Busy waiting** is when a process continually loops in the entry section of a critical section while holding the CPU, checking if a condition has become true. **Other kinds of waiting** include **blocking** or **sleeping**, where the process yields the CPU and is placed in a waiting queue. **Avoidance:** Busy waiting cannot be avoided altogether; for synchronization primitives like spinlocks used for very short waits on multiprocessor systems, busy waiting is often necessary and preferable to the overhead of context switching.||
|**5.7**|A race condition occurs if concurrent execution of `deposit()` and `withdraw()` operations allows one operation to read the balance before the other has finished writing the updated balance, leading to data inconsistency. Prevention requires ensuring **mutual exclusion** by protecting the critical section where the balance is updated, typically using a mechanism like a **mutex lock**.||
|**5.8**|Dekker’s algorithm satisfies: **Mutual Exclusion** (only one process can be in the critical section at a time). **Progress** (if no process is in the critical section, only processes outside their remainder section can determine who enters the critical section next). **Bounded Waiting** (a limit exists on the number of times a process can enter its critical section after another process has indicated its desire to enter).||
|**5.10**|Disabling interrupts should not be used in user-level programs because it gives the user control over the CPU schedule. A malfunctioning or malicious user program could disable interrupts indefinitely, preventing context switches, and halting the entire system, including the clock updating.||
|**5.13**|1. **Process Ready Queue/Run Queue:** A race condition can occur if multiple processors attempt to simultaneously modify the list of ready processes (inserting or deleting a PCB). 2. **List of open files/I/O Status Information** stored in a Process Control Block (PCB): A race condition can occur if two kernel routines attempt to update the list of allocated I/O devices or open files for a process concurrently.||
|**5.15**|The minimal implementation for `acquire()` and `release()` using the structure `typedef struct { int available; } lock;` and the `test_and_set()` instruction is:||
||**acquire():** `while (test_and_set(&lock.available));` (This busy-waits until `available` is set by `test_and_set()` returning `false`). **release():** `lock.available = 0;`||

### Chapter 6: CPU Scheduling (6.1, 6.2, 6.3)

|Question|Minimal Solution & Formula/Principle|Citation|
|:--|:--|:--|
|**6.1**|$\textbf{n!}$ (n factorial).||
||_Formula/Principle:_ The possible schedules represent the total number of unique orderings of $n$ processes, which is $n!$.||
|**6.2**|**Preemptive scheduling** allows a process currently running on the CPU to be interrupted and moved to the ready state (e.g., if a higher-priority process arrives or a time quantum expires). **Nonpreemptive scheduling** requires a process to hold the CPU until it voluntarily releases it (e.g., by terminating or switching to the waiting state for I/O).||
|**6.3**|_Formula/Principle:_ Waiting Time ($W$) for a process is calculated as $W = \text{Start Time} - \text{Arrival Time}$. Average Waiting Time $= \frac{\sum W}{n}$.||
|**6.4**|Using different time quantum sizes (e.g., larger ones at lower priority levels) allows **CPU-bound processes to run efficiently** with fewer costly context switches, while preserving the **fast response time** required by interactive processes (which reside in higher queues with smaller quanta).||
|**6.5**|a. Priority and SJF: **SJF is a special case of Priority** (where priority is the inverse of the predicted next CPU burst length). b. Multilevel feedback queues and FCFS: **FCFS is a possible algorithm used in one of the queues** (usually the lowest priority queue). c. Priority and FCFS: **FCFS is a special case of Priority** (where all priorities are equal). d. RR and SJF: **No direct encompassing relation**.||
|**6.6**|This algorithm **favors I/O-bound programs** because they typically use the least processor time per burst, thus maintaining a perpetually higher implicit priority. It avoids **starvation for CPU-bound programs** if the system employs a mechanism like **aging**, whereby the priority of a waiting process increases over time until it eventually receives the CPU.||
|**6.10**|Schedulers must distinguish them because **I/O-bound programs** have short CPU bursts and often yield the CPU quickly, making them crucial for filling short scheduling gaps and maximizing CPU utilization. **CPU-bound programs** require long stretches of CPU time. Knowing the type of program allows the scheduler to tailor its algorithm (e.g., giving I/O-bound tasks priority) to achieve criteria like better average response time.||
|**6.11**|a. **CPU utilization and response time:** Maximizing CPU utilization may require allowing long jobs to run without preemption, which increases response time for interactive users. b. **Average turnaround time and maximum waiting time:** Algorithms favoring short processes (like SJF) to minimize average turnaround time can indefinitely postpone long processes, resulting in high maximum waiting times and starvation. c. **I/O device utilization and CPU utilization:** These often conflict; if the system favors CPU-bound tasks, I/O devices may sit idle, and vice versa.||
|**6.14**|_Formula/Principle:_ $\tau_{n+1} = \alpha t_n + (1 - \alpha) \tau_n$.||
||If $\alpha = 0$, the predicted length $\tau_{n+1}$ is always equal to the past history $\tau_n$ (recent burst $t_n$ is ignored). If $\alpha = 1$, the predicted length $\tau_{n+1}$ is simply the actual length of the most recent burst $t_n$ (past history $\tau_n$ is ignored).||
|**6.16**|The algorithm resulting in the minimum average waiting time is **SJF** and **Nonpreemptive Priority** (4.6 ms).||
||_Formula/Principle:_ Average Waiting Time $= \frac{\sum W}{n}$. (Calculations based on $W$ values provided in Step 3 of silent reasoning: FCFS=6.2 ms, SJF/Priority=4.6 ms, RR=5.8 ms).||
|**6.19**|**SJF** and **Priority** algorithms could result in starvation.||
|**6.20**|a. Effect of duplicate pointers: The process receives **a greater share of the CPU** (more time slices) in proportion to its number of entries. b. **Advantages:** Simple way to implement priority/share adjustment; higher-priority processes get the CPU more frequently. **Disadvantages:** Increased context switching overhead; complicates tracking of CPU share. c. Modify the algorithm to **dynamically assign a larger time quantum** to the process instead of adding duplicate pointers.||
|**6.22**|The user should ensure the process performs **many short CPU bursts** and blocks frequently, making it appear **I/O-bound**. This behavior typically keeps the process in higher-priority queues of the multilevel queue system, thus maximizing its allocated CPU time.||
|**6.24**|**FCFS:** No discrimination; processes are served strictly in arrival order. **RR:** Discriminates favorably; short processes finish quickly within one time quantum. **Multilevel Feedback Queues (MLFQ):** Discriminates highly favorably; typically places new (short) jobs in the highest priority queue, allowing them to finish without waiting behind long jobs, which are gradually demoted.||

### Chapter 7: Deadlocks (7.1, 7.2, 7.3)

|Question|Minimal Solution & Formula/Principle|Citation|
|:--|:--|:--|
|**7.1**|1. Two cars simultaneously attempting to proceed straight at a four-way intersection when all four paths are blocked. 2. Two people each holding one of two items needed by the other, and neither is willing to relinquish their held item. 3. Two processes competing for two nonpreemptable I/O devices.||
|**7.4**|The **Containment Scheme** prevents the **Hold and Wait** necessary condition by requiring a process to acquire a single, high-order lock (F) before requesting any other subordinate resources (A...E). The **Circular-Wait Scheme** prevents deadlock by imposing a **total ordering** on all resource types, forcing processes to request resources only in increasing order.||
|**7.5**|**Arguments for:** Deadlock avoidance guarantees that the system will never enter a deadlocked state. If the cost of recovery (terminating and rerunning jobs) is high, the avoidance overhead may be justified. **Arguments against:** Deadlock avoidance requires **a priori knowledge** of the maximum resources each process will need during its lifetime. It involves **high runtime overhead** (performing the safety algorithm requires $O(m \times n^2)$ operations per request, where $m$ is resource types and $n$ is processes).||
|**7.10**|**No.** Deadlock requires the **Circular Wait** condition, which inherently involves a chain of two or more waiting processes, where the last process waits for the first (a cycle). A single process cannot form a cycle.||
|**7.11**|**Necessary Conditions:** 1. **Mutual Exclusion:** Intersection blocks (resources) are non-sharable. 2. **Hold and Wait:** Each car holds one block while waiting for the next. 3. **No Preemption:** Cars/resources cannot be forcibly taken. 4. **Circular Wait:** The cars form a cycle, each waiting for the next car to move. **Simple Avoidance Rule:** Do not allow a car to enter the intersection unless the system can guarantee it can **exit the intersection completely** (e.g., unless the desired path is clear).||
|**7.12**|**Yes.** Deadlock is still possible because **writer locks** require exclusive access (mutual exclusion). If multiple writers (or readers waiting for a writer) attempt to acquire multiple locks out of order, the **circular wait** condition can still occur.||
|**7.15**|**Circular-Wait Scheme:** **Runtime Overhead** is low (only checks sequence number). **System Throughput** may be decreased due to constraints on resource requests. **Programming Effort** is required to define and enforce the ordering. **Deadlock-Avoidance Schemes:** **Runtime Overhead** is high (requires performing the safety algorithm at runtime). **System Throughput** is potentially higher than Circular-Wait, but requests may be delayed. **Programming Effort** is low, but requires a priori resource maximums.||
|**7.17**|Total resources $m=4$. Total maximum need is $3 \times 2 = 6$. For deadlock to occur, each of the $n=3$ processes must hold one less than its maximum need, and the remaining resources must be zero. Maximum resources held without guaranteeing termination is $\sum (\text{Max}_i - 1) = (2-1) + (2-1) + (2-1) = 3$. Since 3 resources held leaves $4-3 = 1$ resource available, that resource can be allocated to one waiting process, allowing it to complete and release its resources, thereby preventing deadlock.||