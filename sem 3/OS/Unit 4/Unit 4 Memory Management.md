## Chapter 8: Main Memory

### 8.1.3 Logical Versus Physical Address Space

- An address generated by the CPU is referred to as a **logical address**.
- An address seen by the memory unit (the one loaded into the memory-address register) is the **physical address**.
- The execution-time address-binding scheme results in logical and physical addresses that differ.
    - In this scheme, the logical address is often called a **virtual address** (the terms are used interchangeably).
    - The **logical address space** is the set of all logical addresses generated by a program.
    - The **physical address space** is the set of all physical addresses corresponding to those logical addresses.
- The run-time mapping from virtual to physical addresses is handled by a hardware device called the **memory-management unit (MMU)**.
- A simple MMU scheme generalizes the base-register approach, using a **relocation register**.
    - The value in the relocation register is **added to every address generated by a user process** when the address is sent to memory. For instance, if the base is 14000, an access to logical address 346 is mapped to physical address 14346.
- The concept of a logical address space bound to a separate physical address space is central to proper memory management.

---

### 8.2 Swapping

- A process must be in memory to be executed. **Swapping** allows a process to be **temporarily moved out of memory to a backing store** and later returned to memory for continued execution.
- Swapping enables the total physical address space of all processes to exceed the real physical memory size, thereby increasing the **degree of multiprogramming**.

#### 8.2.1 Standard Swapping

- Standard swapping involves moving processes between main memory and a **backing store** (typically a fast disk).
- The backing store must be large enough to hold copies of all memory images for all users and must provide direct access to these images.
- The system maintains a **ready queue** of all processes ready to run, whether they are on the backing store or already in memory.
- When the CPU scheduler selects a process, the dispatcher checks if it is in memory. If it is not, and if there is no free memory, the dispatcher swaps out a process currently in memory and **swaps in the desired process**.
- Standard swapping is **not used in modern operating systems** because it requires too much time for swapping relative to execution time.
- Modified versions of swapping exist in systems like UNIX, Linux, and Windows, often only starting when the amount of free memory falls below a threshold, or by swapping only portions of processes instead of the entire process.

#### 8.2.2 Swapping on Mobile Systems

- Mobile systems, such as iOS and Android, typically **do not support swapping**.
- This avoidance is due to factors like space constraints (using flash memory instead of hard disks), the limited number of writes flash memory can tolerate, and poor throughput between main memory and flash memory.
- Mobile systems generally support paging.

---

### 8.3 Contiguous Memory Allocation

- **Contiguous memory allocation** is an early method where **each process is contained in a single, contiguous section of memory**.
- Memory is usually partitioned into sections for the resident operating system (often in low memory) and user processes.

#### 8.3.1 Memory Protection

- Protection is achieved using a **relocation register** (holding the smallest legal physical address) and a **limit register** (containing the range of logical addresses).
- The MMU maps the logical address dynamically by adding the relocation register value, and the hardware checks that the logical address falls within the range specified by the limit register.

#### 8.3.2 Memory Allocation

- One method involves dividing memory into **fixed-sized partitions**, where the degree of multiprogramming is fixed by the number of partitions (an old method, historically used by IBM OS/360 MFT).
- A generalization involves solving the **dynamic storage-allocation problem**: satisfying a request of size $n$ from a list of free memory regions ("holes"). Common strategies include:
    - **First fit:** Allocate the first hole found that is big enough.
    - **Best fit:** Allocate the smallest hole that is sufficiently large.
    - **Worst fit:** Allocate the largest hole.

#### 8.3.3 Fragmentation

- Both first-fit and best-fit strategies suffer from **external fragmentation**, where there is enough total memory space to satisfy a request, but the available spaces are not contiguous.
- **Compaction** is one solution to external fragmentation; it shuffles memory contents to bring all free memory together into one large block.
- Compaction is only possible if relocation is **dynamic** (done at execution time).
- Another solution is to allow the logical address space to be **noncontiguous**, accomplished using segmentation or paging.

---

### 8.4 Segmentation

- **Segmentation** is a memory-management scheme that allows the physical address space of a process to be **noncontiguous** and supports the programmer's view of memory.
- Programmers view memory as a collection of **variable-sized segments** (e.g., main program, stack, data structures).
- A logical address space in segmentation is a collection of segments.
- A logical address consists of a two-tuple: **<segment-number, offset>**.

#### 8.4.2 Segmentation Hardware

- Segmentation uses a **segment table** to map two-dimensional logical addresses to one-dimensional physical addresses.
- Each segment table entry includes a **segment base** (starting physical address) and a **segment limit** (length of the segment).
- Address translation checks that the offset ($d$) is within the limit ($0$ to $\text{limit}$). If valid, the offset is added to the segment base to find the physical memory address. If invalid, a trap to the operating system occurs.

---

### 8.5 Paging

- **Paging** is a memory-management scheme that enables a process's physical address space to be **noncontiguous**, avoiding external fragmentation and the need for compaction.
- Paging is implemented through cooperation between the operating system and hardware and is widely used in most operating systems.

#### 8.5.1 Basic Method

- Physical memory is divided into fixed-sized blocks called **frames**.
- Logical memory is divided into blocks of the same size called **pages**.
- A process's pages are loaded into any available frames.
- A logical address is divided into a **page number (p)** and a **page offset (d)**.
- The page number is used as an index into the **page table**, which contains the base address (or frame number, $f$) of each page in physical memory.
- The physical memory address is defined by combining the frame number ($f$) from the page table with the page offset ($d$).
- Page size is a **power of 2** (e.g., 512 bytes to 1 GB, but typically 4 KB to 8 KB today), simplifying the translation of logical addresses into page numbers and offsets.
- The operating system maintains a **frame table** to track physical memory allocation details, such as which frames are free or allocated.
- Internal fragmentation is expected to average one-half page per process.

#### 8.5.2 Hardware Support

- A pointer to the page table is stored in the process control block (PCB).
- To accelerate address translation, a specialized, small, fast-lookup hardware cache called a **translation look-aside buffer (TLB)** is used.
- The TLB is associative, high-speed memory. Each entry contains a key (virtual page number) and a value (frame number).
- A TLB is typically small (32 to 1,024 entries).
- In the event of a TLB miss, the system must consult the page table, which usually resides in main memory.
- Some TLBs allow certain entries, typically for key kernel code, to be **wired down**, meaning they cannot be removed.

## Chapter 9: Virtual Memory Notes

### 9.2 Demand Paging

Demand paging is a core technique used in **virtual memory** systems. Virtual memory is a technique that permits the execution of processes that are **not completely in memory**.

**Key Concepts:**

- **Programs Larger Than Physical Memory:** One major advantage is that programs can be larger than the physical memory available.
- **Lazy Loading:** Demand paging loads pages only **when they are demanded** (referenced) during program execution. Pages that are never accessed are therefore never loaded into physical memory.
- **Performance Benefit:** This approach avoids bringing unused pages into memory, which decreases the swap time and the required amount of physical memory.

**Basic Mechanisms (9.2.1):**

- **Hardware Support:** Hardware must be able to distinguish between pages currently in memory and pages residing on disk.
- **Valid–Invalid Bit:** The hardware utilizes the **valid–invalid bit** (discussed previously in Section 8.5.3).
    - If the bit is "valid," the page is legal and present in memory.
    - If the bit is "invalid," the page is either not valid (outside the logical address space) or is valid but currently residing on the disk.
- **Page Fault:** Accessing a page marked invalid triggers a **page fault** (a trap to the operating system).

**Steps in Handling a Page Fault (Figure 9.6):**

1. Determine the location of the desired page on the disk.
2. Find a **free frame** in memory. If no free frame exists, a **page-replacement algorithm** is used to select a **victim frame**.
3. If the victim frame has been modified (is "dirty"), it is **written out to the disk**; the page and frame tables are updated accordingly.
4. The desired page is read from disk into the newly freed frame, and the page table is reset for the new page.
5. The process is restarted.

**Supporting Demand Paging:**

- **Restartable Instructions:** A crucial requirement is the ability to **restart any instruction** after a page fault, meaning the state of the interrupted process (registers, instruction counter, etc.) must be preserved and restored precisely.
- **Secondary Memory:** Pages not in main memory are held in **secondary memory**, typically a high-speed disk, known as the **swap device**, and the area used is called **swap space**.
- **Mobile Systems:** Mobile operating systems typically **do not support swapping**. Instead, they demand-page from the file system and reclaim read-only pages (like code) from applications if memory becomes constrained.

**Performance (9.2.2):**

- The performance of demand paging is measured by the **effective access time** ($EAT$).
- Typical **memory-access time** ($m_a$) ranges from 10 to 200 nanoseconds.
- If no page faults occur, $EAT$ equals $m_a$.
- If a page fault occurs, a disk access is required, significantly increasing the time.
- The three major components of page-fault service time are:
    1. Service the page-fault interrupt (1 to 100 microseconds).
    2. **Read in the page** (the slowest part, typically **8 milliseconds** for a hard disk).
    3. Restart the process (1 to 100 microseconds).

### 9.4 Page Replacement

**Page replacement** is required when a process incurs a page fault but there are **no free frames** available in physical memory.

**Goals and Necessity:**

- Page replacement algorithms aim to **reduce the page-fault rate**.
- This mechanism completes the separation between logical memory and physical memory, allowing programmers to utilize an **enormous virtual memory** regardless of the actual physical memory size.

**Handling Victim Pages:**

- When a page is selected as a victim, if it has been **modified** (is "dirty"), its contents must be copied back to the disk.
- If the victim page has not been modified, it can simply be discarded.
- A page that has not been modified may be discarded, which **reduces the time to service a page fault by one-half**, as it avoids one I/O operation.

**Algorithm Evaluation:**

- Page replacement algorithms are evaluated by running them on a **reference string** (a sequence of page numbers referenced by the process) and calculating the **number of page faults** generated.
- Generally, as the number of available frames increases, the number of page faults decreases (as illustrated in Figure 9.11).

### 9.4.4 LRU Page Replacement

The **Least Recently Used (LRU) algorithm** attempts to approximate the performance of the infeasible optimal algorithm (OPT).

**Core Principle:**

- The LRU algorithm relies on the principle that the recent past can predict the near future.
- It chooses to replace the page that has **not been used for the longest period of time**.
- In contrast: FIFO uses the time the page was brought into memory; OPT uses the time when the page will be used next.

**Performance Example:**

- For the example reference string (7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1) using three frames, the LRU algorithm results in **12 page faults**.

**Implementation Challenges:**

- Implementing **true LRU** replacement is costly because it requires substantial hardware support.
- **Counter Implementation:** One approach is to associate a **counter** with every page-table entry, updated on every memory reference. The page with the smallest counter value is the victim. This approach is expensive due to the need to update the counter register and copy it to the page table for every memory access, which occurs frequently (every 10 to 200 nanoseconds).
- **Stack Implementation:** Another approach uses a **stack** of page numbers. When a page is referenced, it is moved from its current position to the top of the stack. The page at the bottom is the LRU page. However, updating a stack of $m$ frames requires $O(m)$ time. This overhead is too slow and could theoretically slow down every user process significantly.

Since true LRU is highly difficult to implement efficiently, most systems rely on LRU-approximation algorithms.

---

_Clarification Analogy: Think of LRU replacement like managing a small, frequently accessed toolbox (physical memory). If you need to put a new tool in, you want to get rid of the tool you haven't touched for the longest time, assuming that's the one you least need right now. While this strategy is logically the best (closest to knowing the future), actually tracking the exact moment every tool was last used (down to the nanosecond) would require an observer (hardware) so complex and quick that the time spent watching the tools outweighs the benefit of having them!_